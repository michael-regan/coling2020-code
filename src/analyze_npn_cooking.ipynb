{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Keys:\n",
    "1) 'split' - which dataset split the recipe belongs to\n",
    "2) 'id' - a unique identifier for each recipe\n",
    "3) 'verb' - a dictionary where each key is a step number in the recipe (e.g., '0' is the first step). The value is the list of verbs that occur in this step. If no verbs occur, there should be a \"<NO_CHANGE>\" included\n",
    "4) 'ingredient_list' - a list of plain text words for each ingredient_name\n",
    "5) 'ingredients' - a dictionary where each key is a step number in the recipe. Steps without ingredients don't have a key. The value is a list indices for the ingredients that occur in that step (the indices correspond to their location in ingredient_list)\n",
    "6) 'events' - a dictionary where each key is a step number in the recipe. Steps without an event don't have a key. The value for each key is another dictionary. In this 2nd dictionary, each key is a state change type (i.e., composition, cookedness, etc.). The values in the second dictionary are the end states for each state change type. if a state change type does not exist in this step, the second dictionary does not have a key for it.\n",
    "7) 'text' - a dictionary where each key is a step number in the recipe. Each value is a list of tokens for this step.\n",
    "\n",
    "For \"train\" recipes, two additional keys are as follows (if needed):\n",
    "1) 'ing_type' - a dictionary where each key is a step number in the recipe. The value is a number corresponding to what type of annotation the ingredients are ( 1 = mention annotation, 2 = coref annotation, 3 = no ingredients to be selected)\n",
    "2) 'ingredients_nocoref' - the ingredients at each step without the coref rules to augment training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/research/regan/src/data/cooking_dataset/recipes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_recipes(path):\n",
    "\n",
    "    cnt_files_test = 0\n",
    "    cnt_files_dev = 0\n",
    "    \n",
    "    all_cnt_utts = []\n",
    "    all_cnt_tokens = []\n",
    "    \n",
    "    length_coref_chains_per_file = {}\n",
    "\n",
    "    all_recipes_noun_counts = []\n",
    "    all_vocab_verbs = []\n",
    "\n",
    "\n",
    "    os.chdir(path)\n",
    "\n",
    "    for file in tqdm(glob.glob(\"*.json\")):\n",
    "\n",
    "        length_utts = 0\n",
    "        cnt_utts = 0\n",
    "\n",
    "        # used as proxy for fd-entities\n",
    "        cnt_vocab_nouns_from_ingredients = defaultdict(int)\n",
    "        cnt_vocab_verbs_from_verbs = 0  \n",
    "\n",
    "        with open(file, 'r') as f:\n",
    "\n",
    "            data = json.load(f)\n",
    "\n",
    "            # process length of all utts in 'text'\n",
    "            \n",
    "            if data['split'] in ['test', 'dev']:\n",
    "                text = data['text']\n",
    "                \n",
    "                if data['split']=='test':\n",
    "                    cnt_files_test += 1\n",
    "                    \n",
    "                elif data['split']=='dev':\n",
    "                    cnt_files_dev += 1\n",
    "\n",
    "                for step, utt in text.items():\n",
    "\n",
    "                    length_utts += len(utt)\n",
    "                    cnt_utts += 1\n",
    "\n",
    "                all_cnt_tokens.append(length_utts)\n",
    "                all_cnt_utts.append(cnt_utts)\n",
    "\n",
    "                ingredients = data['ingredients']\n",
    "\n",
    "                for step, arr in ingredients.items():\n",
    "\n",
    "                    for ingred in arr:\n",
    "                        cnt_vocab_nouns_from_ingredients[ingred] += 1\n",
    "\n",
    "                all_recipes_noun_counts.append(cnt_vocab_nouns_from_ingredients)\n",
    "\n",
    "                verbs = data['verb']\n",
    "\n",
    "                for utt_idx, verb_arr in verbs.items():\n",
    "                    cnt_vocab_verbs_from_verbs += len(verb_arr)\n",
    "\n",
    "                all_vocab_verbs.append(cnt_vocab_verbs_from_verbs)\n",
    "\n",
    "\n",
    "    print(\"Number of recipe files in test:\", cnt_files_test)\n",
    "    print(\"Number of recipe files in dev:\", cnt_files_dev)\n",
    "    print(\"Mean number of tokens per file:\", np.mean(all_cnt_tokens))\n",
    "    print(\"Mean number of utts per file:\", np.mean(all_cnt_utts))\n",
    "\n",
    "    length_coref_all_fd_entities = []\n",
    "    cnt_number_unique_fd_entities_per_recipe = []\n",
    "\n",
    "    cnt_number_total_fd_entities_per_recipe = []\n",
    "\n",
    "    for recipe_noun_count in all_recipes_noun_counts:\n",
    "\n",
    "        cnt_number_unique_fd_entities_per_recipe.append(len(recipe_noun_count))\n",
    "        \n",
    "        # each recipe is a defaultdict count the number of entities and their frequencies\n",
    "\n",
    "        this_recipe_coref = []\n",
    "        total_count_fd_entities = 0\n",
    "        for ingred, count in recipe_noun_count.items():\n",
    "            this_recipe_coref.append(count)\n",
    "\n",
    "            total_count_fd_entities += count\n",
    "\n",
    "        cnt_number_total_fd_entities_per_recipe.append(total_count_fd_entities)\n",
    "\n",
    "        if len(this_recipe_coref) > 0:\n",
    "            length_coref_all_fd_entities.append(np.mean(this_recipe_coref))\n",
    "\n",
    "    print(\"Mean number of unique fd-entities per file:\", np.mean(cnt_number_unique_fd_entities_per_recipe))\n",
    "    print(\"Mean number of total fd-entities per file:\", np.mean(cnt_number_total_fd_entities_per_recipe))\n",
    "    print(\"Mean length coref chain per file:\", np.mean(length_coref_all_fd_entities))\n",
    "    print(\"Mean number of verbs per file:\", np.mean(all_vocab_verbs))\n",
    "\n",
    "    # print(\"Number of total nouns:\", len(all_nouns))\n",
    "    # print(\"Mean count of all entities per file:\", np.mean(cnt_all_entities_per_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111229/111229 [00:35<00:00, 3163.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipe files in test: 629\n",
      "Number of recipe files in dev: 162\n",
      "Mean number of tokens per file: 92.69785082174462\n",
      "Mean number of utts per file: 8.833122629582807\n",
      "Mean number of unique fd-entities per file: 8.438685208596713\n",
      "Mean number of total fd-entities per file: 38.78128950695322\n",
      "Mean length coref chain per file: 4.435615955869103\n",
      "Mean number of verbs per file: 11.663716814159292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_recipes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
