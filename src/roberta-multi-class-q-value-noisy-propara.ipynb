{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, roc_curve, auc, roc_auc_score\n",
    "import pickle\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Number of GPU Available: 4\n",
      "GPU: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: {}\".format(torch.cuda.is_available()))\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"Number of GPU Available: {}\".format(n_gpu))\n",
    "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/nfs/research/regan/src/coling2020-code/data/augmented_with_ccs_rq13_v3.csv')\n",
    "\n",
    "df.cc = df.cc.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_q_values = ['COS', 'DES', 'MOT', 'OTHER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_for_train(df, augmented=True):\n",
    "\n",
    "    all_items = []\n",
    "    \n",
    "    if not augmented:\n",
    "        dff = df[df['augmented']==0]\n",
    "    else:\n",
    "        dff = df.copy(deep=True)\n",
    "\n",
    "    for idx, row in dff.iterrows(): \n",
    "\n",
    "        cc = row.cc\n",
    "\n",
    "        for c in cc:\n",
    "            entity = c[\"text\"] \n",
    "            sentence = row.sentence\n",
    "            \n",
    "            sentence = \"<s> \" + sentence + \" </s> </s> \" + entity + \" </s>\"\n",
    "            \n",
    "            prop = c['q_value']\n",
    "            if prop == 'PROP':\n",
    "                prop = 'COS'\n",
    "                \n",
    "            if prop in all_q_values:\n",
    "                pass\n",
    "            else:\n",
    "                prop='OTHER'\n",
    "                \n",
    "            prop_idx = all_q_values.index(prop)\n",
    "\n",
    "            item = {\"sentence\":sentence, 'COS':0, 'DES':0, 'MOT':0, 'OTHER': 0, 'property_index': prop_idx}\n",
    "\n",
    "            q_value = c[\"q_value\"]\n",
    "\n",
    "            if q_value == \"\":\n",
    "                item[\"MOT\"] = 1\n",
    "            elif \"**\" in q_value:\n",
    "                item[\"OTHER\"] = 1\n",
    "            elif \"MPROP\" in q_value or \"PROP\" in q_value:\n",
    "                item[\"COS\"] = 1\n",
    "            elif \"::\" in q_value:\n",
    "                q_values = c[\"q_value\"].split(\"::\")\n",
    "                for q in q_values:\n",
    "                    if q in all_q_values:\n",
    "                        item[q] = 1\n",
    "                    else:\n",
    "                        item['OTHER'] = 1\n",
    "            else:\n",
    "                if q_value in all_q_values:\n",
    "                    item[q_value] = 1\n",
    "                else:\n",
    "                    item['OTHER'] = 1\n",
    "            all_items.append(item)\n",
    "    \n",
    "    return pd.DataFrame(all_items)\n",
    "\n",
    "df_train = make_df_for_train(df, augmented=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>COS</th>\n",
       "      <th>DES</th>\n",
       "      <th>MOT</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>property_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>&lt;s&gt; Dig hole in dirt &lt;/s&gt; &lt;/s&gt; you &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>&lt;s&gt; Dig hole in dirt &lt;/s&gt; &lt;/s&gt; hole &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4545</th>\n",
       "      <td>&lt;s&gt; Make a hole in the wall &lt;/s&gt; &lt;/s&gt; you &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>&lt;s&gt; Make a hole in the wall &lt;/s&gt; &lt;/s&gt; hole &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>&lt;s&gt; Put seed in hole &lt;/s&gt; &lt;/s&gt; you &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>&lt;s&gt; Put seed in hole &lt;/s&gt; &lt;/s&gt; seed &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>&lt;s&gt; Put seed in hole &lt;/s&gt; &lt;/s&gt; hole &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>&lt;s&gt; Place your things on the table &lt;/s&gt; &lt;/s&gt; y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>&lt;s&gt; Place your things on the table &lt;/s&gt; &lt;/s&gt; t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>&lt;s&gt; Place your things on the table &lt;/s&gt; &lt;/s&gt; t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>&lt;s&gt; Inject the fluid into the machine &lt;/s&gt; &lt;/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>&lt;s&gt; Inject the fluid into the machine &lt;/s&gt; &lt;/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>&lt;s&gt; Inject the fluid into the machine &lt;/s&gt; &lt;/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4556</th>\n",
       "      <td>&lt;s&gt; Pour water on seed and hold &lt;/s&gt; &lt;/s&gt; you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>&lt;s&gt; Pour water on seed and hold &lt;/s&gt; &lt;/s&gt; wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>&lt;s&gt; Pour water on seed and hold &lt;/s&gt; &lt;/s&gt; seed...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>&lt;s&gt; Spill the substance onto the earth &lt;/s&gt; &lt;/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>&lt;s&gt; Spill the substance onto the earth &lt;/s&gt; &lt;/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>&lt;s&gt; Spill the substance onto the earth &lt;/s&gt; &lt;/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>&lt;s&gt; Put water on flower &lt;/s&gt; &lt;/s&gt; you &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4563</th>\n",
       "      <td>&lt;s&gt; Put water on flower &lt;/s&gt; &lt;/s&gt; water &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>&lt;s&gt; Put water on flower &lt;/s&gt; &lt;/s&gt; flower &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>&lt;s&gt; Give water to the plants &lt;/s&gt; &lt;/s&gt; you &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>&lt;s&gt; Give water to the plants &lt;/s&gt; &lt;/s&gt; water &lt;/s&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4567</th>\n",
       "      <td>&lt;s&gt; Give water to the plants &lt;/s&gt; &lt;/s&gt; plants ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  COS  DES  MOT  OTHER  \\\n",
       "4543            <s> Dig hole in dirt </s> </s> you </s>    0    0    0      1   \n",
       "4544           <s> Dig hole in dirt </s> </s> hole </s>    0    1    0      0   \n",
       "4545     <s> Make a hole in the wall </s> </s> you </s>    0    0    0      1   \n",
       "4546    <s> Make a hole in the wall </s> </s> hole </s>    0    1    0      0   \n",
       "4547            <s> Put seed in hole </s> </s> you </s>    0    0    0      1   \n",
       "4548           <s> Put seed in hole </s> </s> seed </s>    0    0    0      1   \n",
       "4549           <s> Put seed in hole </s> </s> hole </s>    0    0    0      1   \n",
       "4550  <s> Place your things on the table </s> </s> y...    0    0    0      1   \n",
       "4551  <s> Place your things on the table </s> </s> t...    0    0    0      1   \n",
       "4552  <s> Place your things on the table </s> </s> t...    0    0    0      1   \n",
       "4553  <s> Inject the fluid into the machine </s> </s...    0    0    0      1   \n",
       "4554  <s> Inject the fluid into the machine </s> </s...    0    0    0      1   \n",
       "4555  <s> Inject the fluid into the machine </s> </s...    0    0    0      1   \n",
       "4556  <s> Pour water on seed and hold </s> </s> you ...    0    0    0      1   \n",
       "4557  <s> Pour water on seed and hold </s> </s> wate...    0    0    0      1   \n",
       "4558  <s> Pour water on seed and hold </s> </s> seed...    0    0    0      1   \n",
       "4559  <s> Spill the substance onto the earth </s> </...    0    0    0      1   \n",
       "4560  <s> Spill the substance onto the earth </s> </...    0    0    0      1   \n",
       "4561  <s> Spill the substance onto the earth </s> </...    0    0    0      1   \n",
       "4562         <s> Put water on flower </s> </s> you </s>    0    0    0      1   \n",
       "4563       <s> Put water on flower </s> </s> water </s>    0    0    0      1   \n",
       "4564      <s> Put water on flower </s> </s> flower </s>    0    0    0      1   \n",
       "4565    <s> Give water to the plants </s> </s> you </s>    0    0    0      1   \n",
       "4566  <s> Give water to the plants </s> </s> water </s>    0    0    0      1   \n",
       "4567  <s> Give water to the plants </s> </s> plants ...    0    0    0      1   \n",
       "\n",
       "      property_index  \n",
       "4543               3  \n",
       "4544               1  \n",
       "4545               3  \n",
       "4546               1  \n",
       "4547               3  \n",
       "4548               3  \n",
       "4549               3  \n",
       "4550               3  \n",
       "4551               3  \n",
       "4552               3  \n",
       "4553               3  \n",
       "4554               3  \n",
       "4555               3  \n",
       "4556               3  \n",
       "4557               3  \n",
       "4558               3  \n",
       "4559               3  \n",
       "4560               3  \n",
       "4561               3  \n",
       "4562               3  \n",
       "4563               3  \n",
       "4564               3  \n",
       "4565               3  \n",
       "4566               3  \n",
       "4567               3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    3603\n",
       "0     621\n",
       "2     195\n",
       "1     149\n",
       "Name: property_index, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.property_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(path):\n",
    "\n",
    "    df_test = pd.read_pickle(path)\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "df_test = get_test_data('../data/propara_q_value_for_test_noisy_rq121.pkl')\n",
    "\n",
    "df_test['property_index'] = df_test['property'].apply(lambda x: all_q_values.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>COS</th>\n",
       "      <th>DES</th>\n",
       "      <th>MOT</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>property</th>\n",
       "      <th>property_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt; The sediment and plants are at least one m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>MOT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt; More chemical changes happen and the burie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt; The dead algae and plankton end up part of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt; The dead algae and plankton end up part of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt; The material becomes a liquid. &lt;/s&gt; &lt;/s&gt; m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>COS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  COS  DES  MOT  OTHER  \\\n",
       "0  <s> The sediment and plants are at least one m...    0    0    1      0   \n",
       "1  <s> More chemical changes happen and the burie...    0    1    0      0   \n",
       "2  <s> The dead algae and plankton end up part of...    1    0    0      0   \n",
       "3  <s> The dead algae and plankton end up part of...    1    0    0      0   \n",
       "4  <s> The material becomes a liquid. </s> </s> m...    1    0    0      0   \n",
       "\n",
       "  property  property_index  \n",
       "0      MOT               2  \n",
       "1      DES               1  \n",
       "2      COS               0  \n",
       "3      COS               0  \n",
       "4      COS               0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    701\n",
       "2    543\n",
       "0    334\n",
       "Name: property_index, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.property_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['COS', 'DES', 'MOT', 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "cols = df_train.columns\n",
    "label_cols = list(cols[1:5])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df1, all_labels, batch_size, epochs, lr):\n",
    "\n",
    "    print()\n",
    "    print('Begin train')\n",
    "    print()\n",
    "\n",
    "    labels = list(df1.property_index.values)\n",
    "    num_labels = len(all_labels)\n",
    "    label_counts = df1.property_index.value_counts()\n",
    "    \n",
    "    sentences = list(df1.sentence.values)\n",
    "\n",
    "    print(\"# train_labels:\", len(labels))\n",
    "    print(\"# train_sentences:\", len(sentences))\n",
    "    print()\n",
    "    print(label_counts)\n",
    "    print()\n",
    "\n",
    "    pretrained_weights = 'roberta-large'\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(pretrained_weights, do_lower_case=False, add_special_tokens=False)\n",
    "\n",
    "    train_encodings = tokenizer.batch_encode_plus(sentences,\n",
    "                                            padding=True,\n",
    "                                            return_token_type_ids=True)\n",
    "\n",
    "    print('tokenizer outputs: ', train_encodings.keys())\n",
    "\n",
    "    input_ids = train_encodings['input_ids'] # tokenized and encoded sentences\n",
    "    token_type_ids = train_encodings['token_type_ids']\n",
    "    attention_masks = train_encodings['attention_mask']\n",
    "\n",
    "\n",
    "    # Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n",
    "                                                                random_state=2020, test_size=0.10)\n",
    "\n",
    "    # Convert all of data into tensors\n",
    "    # recall that with the augmented data, I want to define my own validation split; at least initially\n",
    "    # maybe it should not be this way, and use train_test_split()?\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "    validation_token_types = torch.tensor(validation_token_types)\n",
    "\n",
    "\n",
    "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "    # with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(pretrained_weights, num_labels=num_labels)\n",
    "    config.output_hidden_states = True\n",
    "    config.output_attentions = True\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(pretrained_weights, \n",
    "                                                             config=config)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"num_labels:\", model.num_labels)\n",
    "    print()\n",
    "\n",
    "    # setting custom optimization parameters.\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,lr=lr,correct_bias=True)\n",
    "    # optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n",
    "\n",
    "    num_warmup_steps = batch_size\n",
    "    num_total_steps = batch_size * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps) \n",
    "\n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "    train_loss_batch = []\n",
    "    valid_loss_set = []\n",
    "    validation_f1 = []\n",
    "    validation_flat_accuracy = []\n",
    "\n",
    "    # Tracking predictions and labels for confusion matrix\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        # Training\n",
    "\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0 #running loss\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # # Forward pass for multiclass classification\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # logits = outputs[1]\n",
    "\n",
    "            train_loss_set.append(loss.item())    \n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        loss_batch = tr_loss/nb_tr_steps\n",
    "        print(\"Train loss: {}\".format(loss_batch))\n",
    "        train_loss_batch.append(loss_batch)\n",
    "\n",
    "        ###############################################################################\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        val_loss = []\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                # including loss for plots (so adding labels to model definition)\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs[0]\n",
    "                logits = outputs[1]\n",
    "            #Compute loss\n",
    "            #loss_func = CrossEntropyLoss() \n",
    "            #loss = loss_func(logits.view(-1,num_labels),b_labels.long().view(-1))\n",
    "            #loss = loss_func(logits, b_labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            if epoch == epochs-1:\n",
    "                y_pred.extend(np.argmax(logits, axis=1).flatten())\n",
    "                y_true.extend(label_ids.flatten())\n",
    "\n",
    "        avg_val_accuracy = eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = np.mean(val_loss)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "        valid_loss_set.append(float(avg_val_loss))\n",
    "\n",
    "    c_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    print()\n",
    "    print(c_matrix)\n",
    "    print()\n",
    "\n",
    "    print(\"train_loss:\", train_loss_batch)\n",
    "    print(\"valid_loss:\", valid_loss_set)\n",
    "\n",
    "    return model, tokenizer, train_loss_batch, c_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin train\n",
      "\n",
      "# train_labels: 4568\n",
      "# train_sentences: 4568\n",
      "\n",
      "3    3603\n",
      "0     621\n",
      "2     195\n",
      "1     149\n",
      "Name: property_index, dtype: int64\n",
      "\n",
      "tokenizer outputs:  dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 4\n",
      "\n",
      "Train loss: 0.7736165291348169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  12%|█▎        | 1/8 [00:25<02:58, 25.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.80\n",
      "  Validation Loss: 0.70\n",
      "Train loss: 0.40262517029809397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 2/8 [00:51<02:33, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.24\n",
      "Train loss: 0.14338219902196594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  38%|███▊      | 3/8 [01:16<02:07, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-44581f81a7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-eb7ec0b8ee42>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(df1, all_labels, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch-roberta/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/torch-roberta/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 48\n",
    "epochs = 4\n",
    "lr = 3e-5\n",
    "\n",
    "model, tokenizer, train_loss_batch, c_matrix = train(df_train, all_q_values, batch_size, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
